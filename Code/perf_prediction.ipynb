{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "perf_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZoMavn_hBno"
      },
      "source": [
        "# Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEzCUcKrUBxl"
      },
      "source": [
        "!pip install boruta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB0iDa0-hKsB"
      },
      "source": [
        "# Importing libraries and frameworks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eqwMajUuskD"
      },
      "source": [
        "# General python libraries to arrange dataset and plot graphs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import cov\n",
        "from numpy import loadtxt\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Data pre-processing and splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Downloading images\n",
        "from google.colab import files\n",
        "\n",
        "# Measure of accuracy\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import scale \n",
        "\n",
        "# Importing Boruta for feature selection\n",
        "from boruta import BorutaPy\n",
        "\n",
        "# Importing the Random Forest libraries for regression and feature selection\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Importing the LASSO libraries for regression\n",
        "from sklearn.linear_model import Lasso, LassoCV\n",
        "\n",
        "# ANN for regression\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_Ro480ch7sS"
      },
      "source": [
        "# Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV7Uk3s9vHhK"
      },
      "source": [
        "# Import dataset\n",
        "df = pd.read_csv(\"/content/sample_data/dataset.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt2Y-iVAg67h"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zazD73vg1_r"
      },
      "source": [
        "# Remove the 'Name' column from the dataset\n",
        "df = df[df.columns[1:17]]\n",
        "print(df.describe)\n",
        "print(df.dtypes)\n",
        "\n",
        "# Change categorical size to numerical values using One-hot encoding\n",
        "obj_df = df.select_dtypes(include=['object']).copy()\n",
        "ord_enc = OrdinalEncoder()\n",
        "obj_df[\"SizeNumerical\"] = ord_enc.fit_transform(obj_df[[\"Size\"]])\n",
        "obj_df[[\"Size\", \"SizeNumerical\"]].head(11)\n",
        "\n",
        "# Remove the Size column now\n",
        "del obj_df['Size']\n",
        "del df['Size']\n",
        "\n",
        "# Insert the new column size into data frame\n",
        "df.insert(loc=1, column='Size', value=obj_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qElPXgPg47l"
      },
      "source": [
        "# Correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwSwaVLtKqZQ"
      },
      "source": [
        "# Correlation plot to show the interrelation between conditional variables and Exe-time \n",
        "corr = df.corr()\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
        "fig.colorbar(cax)\n",
        "ticks = np.arange(0,len(df.columns))\n",
        "ax.set_xticks(ticks)\n",
        "plt.xticks(rotation=90)\n",
        "ax.set_yticks(ticks)\n",
        "ax.set_xticklabels(df.columns)\n",
        "ax.set_yticklabels(df.columns)\n",
        "\n",
        "# Saving image and displaying the graph\n",
        "# plt.savefig(\"Correlation.png\",bbox_inches = 'tight')\n",
        "# files.download(\"Correlation.png\")\n",
        "plt.show()\n",
        "\n",
        "# The Spearman correlation can evaluate a monotonic relationship between two variables â€” Continous or Ordinal\n",
        "# It is based on the ranked values for each variable rather than the raw data.\n",
        "corrrelation = df.corr(method=\"spearman\");\n",
        "print(\"Spearman rank correlation:\");\n",
        "print(corrrelation);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPV7UM8jCGG-"
      },
      "source": [
        "# Conditional and output variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qO3VHlqztcN"
      },
      "source": [
        "# Split data into X(Conditional variables) and Y(Output variable)\n",
        "X = df.iloc[:, 0:15 ]\n",
        "\n",
        "# Execution time is the output variable. If we can predict the exe-time, we can predict the speedup.\n",
        "Y = df.loc[:, ['Exe-time']]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdCB0Thy-o-e"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mtdD6Mg-zUS"
      },
      "source": [
        "# Random forest feature selection\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X,Y['Exe-time'])\n",
        "\n",
        "# Getting the top 5 features\n",
        "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "feat_importances.nlargest(5).plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQtDHqAmAAcy"
      },
      "source": [
        "# Define Boruta feature selection method\n",
        "feat_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=1)\n",
        "\n",
        "# Find all relevant features\n",
        "feat_selector.fit(X.values, Y.iloc[:, 0])\n",
        "\n",
        "# Check selected features\n",
        "feat_selector.support_\n",
        "\n",
        "# Check ranking of features\n",
        "feat_selector.ranking_\n",
        "\n",
        "# Call transform() on X to filter it down to selected features\n",
        "X_filtered = feat_selector.transform(X.values)\n",
        "\n",
        "# Zip column names, ranks, and decisions in a single iterable\n",
        "feature_ranks = list(zip(df.columns, \n",
        "                         feat_selector.ranking_, \n",
        "                         feat_selector.support_))\n",
        "\n",
        "# Iterate through and print out the results\n",
        "for feat in feature_ranks:\n",
        "    print('Feature: {:<25} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joYmNkTcB9vZ"
      },
      "source": [
        "# Testing and training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXAAGsZWB3Bj"
      },
      "source": [
        "# Random seed value for shuffling\n",
        "seed = 5\n",
        "\n",
        "# 40% test data and 60% training data\n",
        "test_size = 0.4\n",
        "\n",
        "# Prepare testing and training data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsnNUaIBBg37"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Loq8u9lQZFsa"
      },
      "source": [
        "# Random forest Regression\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# Get the mean absolute error on the testing data\n",
        "# Mean Absolute Error is the measure of the difference between the two continuous variables. The MAE is the average vertical distance between each actual value \n",
        "# and the line that best matches the data. MAE is also the average horizontal distance between each data point and the best matching line.\n",
        "predicted_exe_time = model.predict(X_test)\n",
        "MAE = mae(y_test , predicted_exe_time)\n",
        "print('Random forest validation MAE: ', MAE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onVkUKeUZQh5"
      },
      "source": [
        "# LASSO Regression\n",
        "lasso = Lasso(max_iter = 10000, normalize = True)\n",
        "\n",
        "#Setting the alpha value using cross-validation Lasso\n",
        "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
        "lassocv.fit(X_train, y_train)\n",
        "\n",
        "lasso.set_params(alpha=lassocv.alpha_)\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Finding the mean average error of LASSO regression in this dataset\n",
        "print(\"LASSO Validation Mean average error\",mae(y_test, lasso.predict(X_test)))\n",
        "\n",
        "# Finding the R2 score which is a statistical measure of how close the data are to the fitted regression line\n",
        "# The higher the R-squared, the better the model fits the data\n",
        "R2score = r2_score(y_test, lasso.predict(X_test))\n",
        "print(\"R2 score using LASSO Regression\",R2score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6JRauU4ycKT"
      },
      "source": [
        "# Artificial Neural Network\n",
        "\n",
        "# Standardization of dataset\n",
        "# Scale the data values using min-max scaling to improve training\n",
        "# Min-max normalization retains the original distribution of scores except for a scaling factor and transforms all the scores into a common range [0, 1].\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "scaler.fit(X)\n",
        "X_Neural = scaler.transform(X)\n",
        "scaler.fit(Y)\n",
        "Y_Neural = scaler.transform(Y)\n",
        "\n",
        "# Prepare testing and training scaled data for Artificial Neural Network\n",
        "seed = 5\n",
        "test_size = 0.33\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_Neural, Y_Neural, test_size=test_size, random_state=seed)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17MF5k7OK0pm"
      },
      "source": [
        "# Designing the model\n",
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(15, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
        "\n",
        "# The three Hidden Layers :\n",
        "NN_model.add(Dense(10, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(8, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(8, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBO4TV15MOdN"
      },
      "source": [
        "# Running the model\n",
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]\n",
        "NN_model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL4NoIEeMSgW"
      },
      "source": [
        "# Calculating the execution-time using Artificial Neural Network\n",
        "predictions = NN_model.predict(X_test)\n",
        "\n",
        "# Accuracy of the model\n",
        "print(\"Neural Network Mean Absolute Error: \" + str(mae(y_test,predictions)))\n",
        "score = r2_score(y_test, predictions)\n",
        "print(\"R2 score using ANN\",R2score)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}